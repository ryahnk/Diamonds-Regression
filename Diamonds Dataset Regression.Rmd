---
title: "Exploring Regression Techniques - Diamonds Dataset"
author: "Ryan Yahnker"
output: pdf_document
---

```{r setup, include=FALSE}
library(ggplot2)
library(tidyverse)
library(corrplot)
library(faraway)
library(MASS)
library(dplyr)
```

# Set Up

Need to read the dataset, reclassify categorical variables, sample the data, and check that everything has been done correctly.
```{r}
diamonds <- read.csv('Diamonds Prices2022.csv')
head(diamonds)
summary(diamonds)

diamonds$cut = factor(diamonds$cut,
                      levels = c('Fair', 'Good', 'Very Good', 'Premium', 'Ideal'))
diamonds$color = factor(diamonds$color,
                        levels = c('D', 'E', 'F', 'G', 'H', 'I', 'J'))
diamonds$clarity = factor(diamonds$clarity, 
                          levels = c('I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF'))

set.seed(101)

sample_diamonds <- diamonds[sample(1:nrow(diamonds), size = 500, replace = FALSE), ]
str(sample_diamonds)
```

# Regression

Now we look to fit a linear regression model keeping in mind the high correlation of variables found in previous part. 

We will use price as our dependent variable. 
```{r}
diamonds_model <- lm(price ~ depth + cut + carat + color + table, data = sample_diamonds) 
summary(diamonds_model)
```

## Checking for multicolinearity (VIF & Correlation Matrix)

Now that we have fit a model we need to check and adjust the model as need to find the best regression line. 

First, we will check for multicolinearity. One way we can explore this is by calculating the Variance inflation factor (VIF).

Calculating VIF using R, we will use the vif function in the faraway library.

```{r}
faraway::vif(diamonds_model)
```

We can see that the VIF of cutIdeal has a value over 10, which suggests that there could exist multicolinearity.
However, this variable is a dummy variable, so this is not a cause for concern. 

We will now look at other ways to explore multicolinearity in our model.

Another way is using condition index. The square root of the largest eigen value divided by the smallest eigen value 
gives us the condition number. When this number is larger than 30, there could be multicolinearity.

For this we will to use the correlation matrix from our regression equation.

```{r}
diamonds_matrix <- model.matrix(diamonds_model)[,-1]
diamonds_corr <- cor(diamonds_matrix)
eigenvalue <- eigen(diamonds_corr)$values
sqrt(max(eigenvalue)/eigenvalue)
```

We can see here that there are no values over 30, so we will explore other options for testing our model.

## Stepwise Elemination

We will now use stepwise elimination to identify the most relevant predictors for the model.
These methods help to avoid overfitting leaving out predictors that are not significantly increasing the accuracy of the model. 

We will check both forward and backward stepwise elimination to see if we get the same result. We will use a k value of 2, because this is the standard k value for minimizing prediction error in models.

```{r}
diamonds_stepB <- stepAIC(diamonds_model, direction="both", k = 2, trace = 0)
summary(diamonds_stepB)
```

Backwards stepwise elimination yields a final model includes only the variables cut, carat, and color.

This model appears to be slightly better than our original diamonds_model because the Adjusted R-squared value is slightly higher, with a value of 0.837 rather than 0.8367. This is a minor improvement, but and improvement nonetheless. 

We will now confirm this adjusted model with forward stepwise elimination to see if we get the same result. 

```{r}
empty_model <- lm(price ~ 1, data=sample_diamonds)
diamonds_stepF <- stepAIC(empty_model, direction="both",
                          scope=list(upper=formula(diamonds_model)), k = 2, trace = 0)
summary(diamonds_stepF)
```

This yields the same result as diamonds_stepB, further suggesting that a better model only includes cut, carat, and color.

## Cross Validation

We can check if our new model results in less Mean Squared Error using Cross Validation. 

We will do so by creating a CV function, then applying it to our two models to check each models MSE. 

```{r}
kFoldCV <- function(data, response, formula, k, seed = FALSE, seed_num, shuffle = FALSE) {
  if (seed == TRUE) {
    set.seed(seed_num)
  }
  if(shuffle == TRUE) {
    shuffle_index = sample(1:nrow(data), replace = FALSE)
    data = data[shuffle_index,]
    response = response[shuffle_index]
  }
  folds <- cut(seq(1,nrow(data)),breaks=k,labels=FALSE)
  mse = numeric()
  for(i in 1:k){
    testIndexes <- which(folds == i, arr.ind=TRUE)
    testData <- data[testIndexes, ]
    trainData <- data[-testIndexes, ]

    diamonds_model.train = lm(formula, data = trainData)
    mse[i] = (1/length(testIndexes))*sum((response[testIndexes]
                                          - predict(diamonds_model.train, newdata = testData))^2)
  }
  rmse = sqrt(mse)
  cv_k_mse = sum(mse)/k
  cv_k_rmse = sum(rmse)/k
  return(list(CV_MSE = cv_k_mse, CV_RMSE = cv_k_rmse))
}

kFoldCV(response = diamonds$price, formula =price ~ depth + cut + carat + color + table,
        data = diamonds, k = 10, shuffle = TRUE, seed = TRUE, seed_num = 1)
```


```{r}
kFoldCV(response = diamonds$price, formula = price ~ cut + carat + color, data = diamonds,
        k = 10, shuffle = TRUE, seed = TRUE, seed_num = 1)
```

Cross validation is giving us an interesting result. We can see that MSE for our simplified model is higher than the MSE for our original model.

The results of cross validation pose need for a look into the correlation between depth and price and table and price. 

For this we can use a corr plot. 

```{r}
correlation_diamonds <- cor(sample_diamonds %>% dplyr::select_if(is.numeric) %>% 
                              dplyr::select(-X, -x, -y, -z, -carat))

corrplot(correlation_diamonds, method="color", 
         col=colorRampPalette(c("green", "white", "red"))(200),
         type="upper", tl.col="black", tl.srt=45, addCoef.col = "black")
```

```{r}
pairs(sample_diamonds[, c('table', 'depth', 'price')])
```

It appears that neither table, depth, or price are significantly correlated. It seems strange that MSE increases when they are removed from the model. 

It is possible the seed_num used is giving an unlucky result. Since the MSE increase is strange, let's explore other seeds.

```{r}
kFoldCV(response = diamonds$price, formula =price ~ depth + cut + carat + color + table,
        data = diamonds, k = 10, shuffle = TRUE, seed = TRUE, seed_num = 43)

kFoldCV(response = diamonds$price, formula =price ~ cut + carat + color,
        data = diamonds, k = 10, shuffle = TRUE, seed = TRUE, seed_num = 43)

```

```{r}
kFoldCV(response = diamonds$price, formula =price ~ depth + cut + carat + color + table,
        data = diamonds, k = 10, shuffle = TRUE, seed = TRUE, seed_num = 713)

kFoldCV(response = diamonds$price, formula =price ~ cut + carat + color,
        data = diamonds, k = 10, shuffle = TRUE, seed = TRUE, seed_num = 713)

```

```{r}
kFoldCV(response = diamonds$price, formula =price ~ depth + cut + carat + color + table,
        data = diamonds, k = 10, shuffle = TRUE, seed = TRUE, seed_num = 123)

kFoldCV(response = diamonds$price, formula =price ~ cut + carat + color,
        data = diamonds, k = 10, shuffle = TRUE, seed = TRUE, seed_num = 123)

```

```{r}
kFoldCV(response = diamonds$price, formula =price ~ depth + cut + carat + color + table,
        data = diamonds, k = 10, shuffle = TRUE, seed = TRUE, seed_num = 600493)

kFoldCV(response = diamonds$price, formula =price ~ cut + carat + color,
        data = diamonds, k = 10, shuffle = TRUE, seed = TRUE, seed_num = 600493)

```

## Conclusion

After trying multiple seeds, the result is showing the same. It is still possible this is a sampling error, since there is not significant correlation between price, table, and depth. For now we will assume there is a sampling error and conclude that a model with only the predictors cut, carat, and color is better due to a larger adjusted R^2 value. 










